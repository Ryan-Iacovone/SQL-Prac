{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to microsoft sql db (MSSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the required package if you haven't already\n",
    "# pip install pyodbc\n",
    "# Install sqlalchemy if you haven't already\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "\n",
    "# Create connection string for SQLAlchemy\n",
    "params = urllib.parse.quote_plus(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=localhost\\\\SQLEXPRESS;'\n",
    "    'Trusted_Connection=yes;'\n",
    "    'DATABASE=TestDB;'  # Specify database directly in connection\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={params}')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    customers = pd.read_sql_query(text('SELECT * FROM Customers'), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather and save video game data from unofficial overwatch API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "import os\n",
    "\n",
    "platform = \"pc\"\n",
    "user = \"wildwolf-199415\"\n",
    "\n",
    "url = f\"https://ow-api.com/v1/stats/{platform}/us/{user}/complete\"\n",
    "\n",
    "# Send the HTTP GET request to the API and parse the JSON response\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "save_folder = r\"C:\\Users\\Ryan\\Coding Projects\\SQL\"\n",
    "file_path = os.path.join(save_folder, 'overwatch_db.json')\n",
    "\n",
    "# Writing the json file \n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "# Opening the json file\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Overwatch db from local json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = r\"C:\\Users\\Ryan\\Coding Projects\\SQL Prac\\overwatch_db.json\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwatch_db[\"competitiveStats\"][\"topHeroes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pandas df to postgressql function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual function saving\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def df_to_postgresql(df: pd.DataFrame, name: str, schema: str, conn_str: str) -> None:\n",
    "\n",
    "    # Input validation for function\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"df must be a pandas DataFrame\")\n",
    "    if not isinstance(name, str):\n",
    "        raise TypeError(\"name must be a string\")\n",
    "    if not isinstance(schema, str):\n",
    "        raise TypeError(\"schema must be a string\")\n",
    "\n",
    "    # Load variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create an SQLAlchemy engine\n",
    "    engine = create_engine(os.getenv(conn_str))\n",
    "\n",
    "    # name parameter sets the table name, Schema directs to db schema (must be made first in sql)\n",
    "    df.to_sql(name=name, con=engine, schema=schema, if_exists='replace', index=True)\n",
    "\n",
    "    # After using the engine to interact with the database\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving QP top heros to db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "temp_json = overwatch_db['quickPlayStats']['topHeroes']\n",
    "\n",
    "QP_Top_Heros = pd.DataFrame(temp_json)\n",
    "\n",
    "# Simple transpose to flip columns and rows\n",
    "QP_Top_Heros =  QP_Top_Heros.T\n",
    "\n",
    "# Putting the index as a regular column in DF \n",
    "QP_Top_Heros = QP_Top_Heros.reset_index()\n",
    "# Renaming that index column to hero\n",
    "QP_Top_Heros.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "\n",
    "del temp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the QP_Top_Heros df to an postgres db\n",
    "df_to_postgresql(QP_Top_Heros, \"top_heros\", \"qp\", \"postgres_cs_ow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving QP career stats df to postgres db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and transforming\n",
    "t_json = overwatch_db['quickPlayStats']['careerStats']\n",
    "\n",
    "# Changing dict/json to df and transposing it to flip columns and rows\n",
    "qp_career_stats = pd.DataFrame(t_json).T\n",
    "\n",
    "del t_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the dictionary from each column except for \"heroSpecific\" and make each dict pairing into their own column and row  \n",
    "def expand_dict(qp_career_stats):\n",
    "\n",
    "    # Put all column names from df into a list  \n",
    "    dict_columns = qp_career_stats.columns.to_list()\n",
    "\n",
    "    # Deleting heroSpecific from the column list (will be it's own db)\n",
    "    del dict_columns[4]\n",
    "\n",
    "    for col in dict_columns:\n",
    "        # Expand each dictionary column by using a pd.Series to create a new row for each dictionary it encounters with the dictionary keys as column names and values as the row data.pandas series\n",
    "        expanded_cols = qp_career_stats[col].apply(lambda x: pd.Series(x, dtype=\"object\"))\n",
    "        \n",
    "        # Rename new columns to include original column name as a prefix\n",
    "        expanded_cols = expanded_cols.add_prefix(f\"{col}_\")\n",
    "        \n",
    "        # Concatenate the expanded columns with the original DataFrame\n",
    "        qp_career_stats = pd.concat([qp_career_stats, expanded_cols], axis=1)\n",
    "\n",
    "        # Drop the original dictionary column\n",
    "        qp_career_stats = qp_career_stats.drop(columns=[col])\n",
    "    \n",
    "    # Putting the index as a regular column in DF \n",
    "    qp_career_stats = qp_career_stats.reset_index()\n",
    "    # Renaming that index column to hero\n",
    "    qp_career_stats.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "\n",
    "    # dropping the heroSpecific column to be used in later db (SQL can't handle dictionaries in columns)\n",
    "    qp_career_stats = qp_career_stats.drop(columns=[\"heroSpecific\"])\n",
    "    \n",
    "    return qp_career_stats\n",
    "\n",
    "qp_career_stats = expand_dict(qp_career_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Quick play career stats data to db\n",
    "df_to_postgresql(qp_career_stats, \"career_stats\", \"qp\", \"postgres_cs_ow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving heroSpecific df to postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Data cleaning\n",
    "t_json = overwatch_db['quickPlayStats']['careerStats']\n",
    "\n",
    "# Changing dict/json to df and transposing it to flip columns and rows\n",
    "qp_career_stats = pd.DataFrame(t_json).T\n",
    "\n",
    "# Putting the index as a regular column in DF \n",
    "qp_career_stats = qp_career_stats.reset_index()\n",
    "\n",
    "# Renaming that index column to hero\n",
    "qp_career_stats.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "\n",
    "# Getting each hero and their specific stats into a df, getting rid of first observation because that's all heros\n",
    "hero_specific = qp_career_stats[['Hero', 'heroSpecific']].iloc[1:]\n",
    "\n",
    "del qp_career_stats, t_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop puts each heros specific stats into a unique df named after the hero then saves it postgres\n",
    "for index, row in hero_specific.iterrows():\n",
    "    hero = row['Hero']\n",
    "    hero_dict = row['heroSpecific']\n",
    "\n",
    "    # Convert the dictionary into a new DataFrame\n",
    "    new_df = pd.DataFrame([hero_dict])\n",
    "\n",
    "    # Saves each newly created hero df to postgres db\n",
    "    df_to_postgresql(new_df, hero, \"qp\", \"postgres_cs_ow\")\n",
    "\n",
    "    # Not best practice but I included the engine.dispose() above so it keeps connecting and disconnecting for each hero entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in postgres db to pandas df\n",
    "If schema or table name is case sensitive, need to put \"\" around it!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create an SQLAlchemy engine, make sure to update env file with correct db \n",
    "engine = create_engine(os.getenv(\"postgres_cs_ow\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(\n",
    "        text('SELECT * FROM qp.ana'),\n",
    "        connection)\n",
    "\n",
    "# Using specific columns and conditions\n",
    "with engine.connect() as connection:\n",
    "    query = text(\"\"\"\n",
    "        SELECT card_number, name, color, print_completed\n",
    "        FROM archive\n",
    "        WHERE color = :color\"\"\")\n",
    "    \n",
    "    df_filtered = pd.read_sql_query(\n",
    "        query,\n",
    "        connection,\n",
    "        params={\"color\": \"Blue\"})\n",
    "\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, modify local, Update SQL db (3D printing workflow example)\n",
    "\n",
    "Hypothetical: read in data from sql, then execute patron_contacting email script from 3d printing, results in new df called df_filtered, update existing sql db with df_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Read in data from sql with specific paramters\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading keys in\n",
    "load_dotenv()\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(os.getenv(\"postgres_cs_3d\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df_filter = pd.read_sql_query(\n",
    "        text(\"\"\"SELECT * FROM archive\n",
    "                WHERE print_completed = 'X'\n",
    "                AND patron_contacted IS NULL\n",
    "                AND invalid_email IS NULL\n",
    "                AND picked_up IS NULL\"\"\"),\n",
    "        connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mimick sending out emails to patrons\n",
    "# Results in modified df -> df_filter\n",
    "\n",
    "import datetime as dt\n",
    "# Assuming `df_filter` is a pandas DataFrame\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"patron_contacted\"] = \"X\"\n",
    "\n",
    "today_str = dt.date.today().strftime(\"%m/%d/%y\") \n",
    "today_date = dt.datetime.strptime(today_str, \"%m/%d/%y\").date()\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"contacted_date\"] = today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Update Postgres db based on new inputs from df_filtered \n",
    "\n",
    "# Update query\n",
    "update_query = \"\"\"\n",
    "    UPDATE archive \n",
    "    SET patron_contacted = :patron_contacted,\n",
    "        contacted_date = :contacted_date,\n",
    "        invalid_email = :invalid_email\n",
    "    WHERE card_number = :card_number\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame rows to list of dictionaries for batch update (connection.execute used dictionaries as input for the parameters function)\n",
    "records_to_update = df_filter.to_dict('records')\n",
    "\n",
    "# Establish connection and execute updates\n",
    "with engine.begin() as connection:\n",
    "    try:\n",
    "        for record in records_to_update:\n",
    "            connection.execute(\n",
    "                text(update_query),\n",
    "                parameters=record)\n",
    "\n",
    "        print(f\"Successfully updated {len(records_to_update)} records in the database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating database: {str(e)}\")\n",
    "        connection.rollback() # If an error occurs during the database update process, any partial or uncommitted changes made during the transaction are undone.\n",
    "        raise\n",
    "\n",
    "# Release and clean up all database connections managed by the SQLAlchemy engine \n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MYSQL db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_mysql_db(db): \n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # Define your credentials\n",
    "    username = 'root'\n",
    "    password = quote_plus(os.getenv(\"mysql_pass\"))  # Encodes the special characters\n",
    "    host = 'localhost' \n",
    "    port = 3306\n",
    "    database = db\n",
    "\n",
    "    # Create the connection string\n",
    "    connection_string = f\"mysql+pymysql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Test the connection\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # Use pd.read_sql_query to fetch data\n",
    "            df = pd.read_sql_query(text(\"SELECT * FROM city\"), connection)\n",
    "            print(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_mysql_db(\"sakila\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferior way to connecting to mysql\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",\n",
    "    password = os.getenv(\"mysql_pass\") ,\n",
    "    database = \"sakila\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM city\")\n",
    "results = cursor.fetchall()\n",
    "print(results)\n",
    "\n",
    "cursor.close()\n",
    "conn.close() \n",
    "\n",
    "# Using pandas read_sql function is untested outside SQLAlchemy, so it'll throw a warning but still work\n",
    "# df = pd.read_sql(\"SELECT * FROM city\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "postgres_engine = create_engine(os.getenv(\"postgres_cs_nba\"))\n",
    "\n",
    "# SQLite connection\n",
    "sqlite_file  = \"nba_stats.sqlite\"\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "# Query to select a table\n",
    "table_name = 'common_player_info'\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "# Load the table into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random excel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints = pd.read_excel(r\"C:\\Users\\Ryan\\Desktop\\EGR KDL Master 3D Printing List.xlsx\")\n",
    "\n",
    "df_to_postgresql(prints, \"archive\", \"public\", \"postgres_cs_3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_save = (r\"C:\\Users\\Ryan\\Desktop\")\n",
    "file_name = f\"qp_career_stats.xlsx\" \n",
    "file_path = os.path.join(directory_save, file_name) \n",
    "\n",
    "qp_career_stats.to_excel(file_path , index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlalchemy + pandas is the way to go for interacting with any SQL db then modifying it with pandas    \n",
    "psycopg2=postgres  &  pymysql=mysql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
