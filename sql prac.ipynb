{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to microsoft sql db (MSSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the required package if you haven't already\n",
    "# pip install pyodbc\n",
    "# Install sqlalchemy if you haven't already\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "\n",
    "# Create connection string for SQLAlchemy\n",
    "params = urllib.parse.quote_plus(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=localhost\\\\SQLEXPRESS;'\n",
    "    'Trusted_Connection=yes;'\n",
    "    'DATABASE=TestDB;'  # Specify database directly in connection\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={params}')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    customers = pd.read_sql_query(text('SELECT * FROM Customers'), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather and save video game data from unofficial overwatch API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "import os\n",
    "\n",
    "platform = \"pc\"\n",
    "user = \"wildwolf-199415\"\n",
    "\n",
    "url = f\"https://ow-api.com/v1/stats/{platform}/us/{user}/complete\"\n",
    "\n",
    "# Send the HTTP GET request to the API and parse the JSON response\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "save_folder = r\"C:\\Users\\Ryan\\Coding Projects\\SQL\"\n",
    "file_path = os.path.join(save_folder, 'overwatch_db.json')\n",
    "\n",
    "# Writing the json file \n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "# Opening the json file\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Overwatch db from local json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = r\"C:\\Users\\Ryan\\Coding Projects\\SQL Prac\\overwatch_db.json\"\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overwatch_db[\"competitiveStats\"][\"topHeroes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining pandas df to postgressql function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual function saving\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def df_to_postgresql(df: pd.DataFrame, name: str, schema: str, conn_str: str) -> None:\n",
    "\n",
    "    # Input validation for function\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"df must be a pandas DataFrame\")\n",
    "    if not isinstance(name, str):\n",
    "        raise TypeError(\"name must be a string\")\n",
    "    if not isinstance(schema, str):\n",
    "        raise TypeError(\"schema must be a string\")\n",
    "\n",
    "    # Load variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create an SQLAlchemy engine\n",
    "    engine = create_engine(os.getenv(conn_str))\n",
    "\n",
    "    # name parameter sets the table name, Schema directs to db schema (must be made first in sql)\n",
    "    df.to_sql(name = name, con=engine, schema= schema, if_exists='replace', index=True)\n",
    "\n",
    "    # After using the engine to interact with the database\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving QP top heros to db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning \n",
    "temp_json = overwatch_db['quickPlayStats']['topHeroes']\n",
    "\n",
    "QP_Top_Heros = pd.DataFrame(temp_json)\n",
    "\n",
    "# Simple transpose to flip columns and rows\n",
    "QP_Top_Heros =  QP_Top_Heros.T\n",
    "\n",
    "del temp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the QP_Top_Heros df to an postgres db\n",
    "df_to_postgresql(QP_Top_Heros, \"Top_heros\", \"QP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving QP career stats df to postgres db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and transforming\n",
    "t_json = overwatch_db['quickPlayStats']['careerStats']\n",
    "\n",
    "# Changing dict/json to df and transposing it to flip columns and rows\n",
    "qp_career_stats = pd.DataFrame(t_json).T\n",
    "\n",
    "del t_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the dictionary from each column except for \"heroSpecific\" and make each dict pairing into their own column and row  \n",
    "def expand_dict(qp_career_stats):\n",
    "\n",
    "    # Put all column names from df into a list  \n",
    "    dict_columns = qp_career_stats.columns.to_list()\n",
    "\n",
    "    # Deleting heroSpecific from the column list (will be it's own db)\n",
    "    del dict_columns[4]\n",
    "\n",
    "    for col in dict_columns:\n",
    "        # Expand each dictionary column by using a pd.Series to create a new row for each dictionary it encounters with the dictionary keys as column names and values as the row data.pandas series\n",
    "        expanded_cols = qp_career_stats[col].apply(lambda x: pd.Series(x, dtype=\"object\"))\n",
    "        \n",
    "        # Rename new columns to include original column name as a prefix\n",
    "        expanded_cols = expanded_cols.add_prefix(f\"{col}_\")\n",
    "        \n",
    "        # Concatenate the expanded columns with the original DataFrame\n",
    "        qp_career_stats = pd.concat([qp_career_stats, expanded_cols], axis=1)\n",
    "\n",
    "        # Drop the original dictionary column\n",
    "        qp_career_stats = qp_career_stats.drop(columns=[col])\n",
    "    \n",
    "    # Putting the index as a regular column in DF \n",
    "    qp_career_stats = qp_career_stats.reset_index()\n",
    "    # Renaming that index column to hero\n",
    "    qp_career_stats.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "\n",
    "    # dropping the heroSpecific column to be used in later db (SQL can't handle dictionaries in columns)\n",
    "    qp_career_stats = qp_career_stats.drop(columns=[\"heroSpecific\"])\n",
    "    \n",
    "    return qp_career_stats\n",
    "\n",
    "qp_career_stats = expand_dict(qp_career_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Quick play career stats data to db\n",
    "df_to_postgresql(qp_career_stats, \"Career_stats\", \"QP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving heroSpecific df to postgres db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Data cleaning\n",
    "t_json = overwatch_db['quickPlayStats']['careerStats']\n",
    "\n",
    "# Changing dict/json to df and transposing it to flip columns and rows\n",
    "qp_career_stats = pd.DataFrame(t_json).T\n",
    "\n",
    "# Putting the index as a regular column in DF \n",
    "qp_career_stats = qp_career_stats.reset_index()\n",
    "\n",
    "# Renaming that index column to hero\n",
    "qp_career_stats.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "\n",
    "# Getting each hero and their specific stats into a df, getting rid of first observation because that's all heros\n",
    "hero_specific = qp_career_stats[['Hero', 'heroSpecific']].iloc[1:]\n",
    "\n",
    "del qp_career_stats, t_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop puts each heros specific stats into a unique df named after the hero then saves it postgres\n",
    "for index, row in hero_specific.iterrows():\n",
    "    hero = row['Hero']\n",
    "    hero_dict = row['heroSpecific']\n",
    "\n",
    "    # Convert the dictionary into a new DataFrame\n",
    "    new_df = pd.DataFrame([hero_dict])\n",
    "\n",
    "    # Saves each newly created hero df to postgres db\n",
    "    df_to_postgresql(new_df, hero, \"QP\")\n",
    "\n",
    "    # Not best practice but I included the engine.dispose() above so it keeps connecting and disconnecting for each hero entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in postgres db to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Create an SQLAlchemy engine, make sure to update env file with correct db \n",
    "engine = create_engine(os.getenv(\"postgres_cs_3d\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(\n",
    "        text(\"SELECT * FROM archive\"),\n",
    "        connection)\n",
    "\n",
    "# Using specific columns and conditions\n",
    "with engine.connect() as connection:\n",
    "    query = text(\"\"\"\n",
    "        SELECT card_number, name, color, print_completed\n",
    "        FROM archive\n",
    "        WHERE color = :color\"\"\")\n",
    "    \n",
    "    df_filtered = pd.read_sql_query(\n",
    "        query,\n",
    "        connection,\n",
    "        params={\"color\": \"Blue\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read, modify local, Update SQL db (3D printing workflow example)\n",
    "\n",
    "Hypothetical: read in data from sql, then execute patron_contacting email script from 3d printing, results in new df called df_filtered, update existing sql db with df_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Read in data from sql with specific paramters\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Loading keys in\n",
    "load_dotenv()\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(os.getenv(\"postgres_cs_3d\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df_filter = pd.read_sql_query(\n",
    "        text(\"\"\"SELECT * FROM archive\n",
    "                WHERE print_completed = 'X'\n",
    "                AND patron_contacted IS NULL\n",
    "                AND invalid_email IS NULL\n",
    "                AND picked_up IS NULL\"\"\"),\n",
    "        connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mimick sending out emails to patrons\n",
    "# Results in modified df -> df_filter\n",
    "\n",
    "import datetime as dt\n",
    "# Assuming `df_filter` is a pandas DataFrame\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"patron_contacted\"] = \"X\"\n",
    "\n",
    "today_str = dt.date.today().strftime(\"%m/%d/%y\") \n",
    "today_date = dt.datetime.strptime(today_str, \"%m/%d/%y\").date()\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"contacted_date\"] = today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Update Postgres db based on new inputs from df_filtered \n",
    "\n",
    "# Update query\n",
    "update_query = \"\"\"\n",
    "    UPDATE archive \n",
    "    SET patron_contacted = :patron_contacted,\n",
    "        contacted_date = :contacted_date,\n",
    "        invalid_email = :invalid_email\n",
    "    WHERE card_number = :card_number\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame rows to list of dictionaries for batch update (connection.execute used dictionaries as input for the parameters function)\n",
    "records_to_update = df_filter.to_dict('records')\n",
    "\n",
    "# Establish connection and execute updates\n",
    "with engine.begin() as connection:\n",
    "    try:\n",
    "        for record in records_to_update:\n",
    "            connection.execute(\n",
    "                text(update_query),\n",
    "                parameters=record)\n",
    "\n",
    "        print(f\"Successfully updated {len(records_to_update)} records in the database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating database: {str(e)}\")\n",
    "        connection.rollback() # If an error occurs during the database update process, any partial or uncommitted changes made during the transaction are undone.\n",
    "        raise\n",
    "\n",
    "# Release and clean up all database connections managed by the SQLAlchemy engine \n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MYSQL db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection failed: 'cryptography' package is required for sha256_password or caching_sha2_password auth methods\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 36\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_mysql_db\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msakila\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mload_mysql_db\u001b[1;34m(db)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_mysql_db(db): \n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # Define your credentials\n",
    "    username = 'root'\n",
    "    password = quote_plus(os.getenv(\"mysql_pass\"))  # Encodes the special characters\n",
    "    host = 'localhost' \n",
    "    port = 3306\n",
    "    database = db\n",
    "\n",
    "    # Create the connection string\n",
    "    connection_string = f\"mysql+pymysql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Test the connection\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # Use pd.read_sql_query to fetch data\n",
    "            df = pd.read_sql_query(text(\"SELECT * FROM city\"), connection)\n",
    "            print(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_mysql_db(\"sakila\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "'cryptography' package is required for sha256_password or caching_sha2_password auth methods",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymysql\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quote_plus\n\u001b[1;32m----> 4\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmysql_pass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msakila\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     12\u001b[0m cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM city\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pymysql\\connections.py:361\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pymysql\\connections.py:669\u001b[0m, in \u001b[0;36mConnection.connect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_seq_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_server_information()\n\u001b[1;32m--> 669\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_authentication\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;66;03m# Send \"SET NAMES\" query on init for:\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# - Ensure charaset (and collation) is set to the server.\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;66;03m#   - collation_id in handshake packet may be ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;66;03m# - https://github.com/wagtail/wagtail/issues/9477\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;66;03m# - https://zenn.dev/methane/articles/2023-mysql-collation (Japanese)\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_character_set(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollation)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pymysql\\connections.py:979\u001b[0m, in \u001b[0;36mConnection._request_authentication\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;66;03m# https://dev.mysql.com/doc/internals/en/successful-authentication.html\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auth_plugin_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaching_sha2_password\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 979\u001b[0m     auth_packet \u001b[38;5;241m=\u001b[39m \u001b[43m_auth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaching_sha2_password_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_packet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auth_plugin_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msha256_password\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    981\u001b[0m     auth_packet \u001b[38;5;241m=\u001b[39m _auth\u001b[38;5;241m.\u001b[39msha256_password_auth(\u001b[38;5;28mself\u001b[39m, auth_packet)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pymysql\\_auth.py:267\u001b[0m, in \u001b[0;36mcaching_sha2_password_auth\u001b[1;34m(conn, pkt)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;28mprint\u001b[39m(conn\u001b[38;5;241m.\u001b[39mserver_public_key\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 267\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msha2_rsa_encrypt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msalt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_public_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m pkt \u001b[38;5;241m=\u001b[39m _roundtrip(conn, data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pymysql\\_auth.py:144\u001b[0m, in \u001b[0;36msha2_rsa_encrypt\u001b[1;34m(password, salt, public_key)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"Encrypt password with salt and public_key.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mUsed for sha256_password and caching_sha2_password.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _have_cryptography:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcryptography\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is required for sha256_password or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m caching_sha2_password auth methods\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m     )\n\u001b[0;32m    148\u001b[0m message \u001b[38;5;241m=\u001b[39m _xor_password(password \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, salt)\n\u001b[0;32m    149\u001b[0m rsa_key \u001b[38;5;241m=\u001b[39m serialization\u001b[38;5;241m.\u001b[39mload_pem_public_key(public_key, default_backend())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 'cryptography' package is required for sha256_password or caching_sha2_password auth methods"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password= quote_plus(os.getenv(\"mysql_pass\")) ,\n",
    "    database=\"sakila\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM city\")\n",
    "results = cursor.fetchall()\n",
    "print(results)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Using pandas read_sql function is untested outside SQLAlchemy, so it'll throw a warning but still work\n",
    "# df = pd.read_sql(\"SELECT * FROM city\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random excel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints = pd.read_excel(r\"C:\\Users\\Ryan\\Desktop\\EGR KDL Master 3D Printing List.xlsx\")\n",
    "\n",
    "df_to_postgresql(prints, \"archive\", \"public\", \"postgres_cs_3d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_save = (r\"C:\\Users\\Ryan\\Desktop\")\n",
    "file_name = f\"qp_career_stats.xlsx\" \n",
    "file_path = os.path.join(directory_save, file_name) \n",
    "\n",
    "qp_career_stats.to_excel(file_path , index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlalchemy + pandas is the way to go for interacting with any SQL db then modifying it with pandas    \n",
    "psycopg2=postgres  &  pymysql=mysql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
