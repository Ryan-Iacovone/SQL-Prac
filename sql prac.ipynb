{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to microsoft sql db (MSSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, install the required package if you haven't already\n",
    "# pip install pyodbc\n",
    "# Install sqlalchemy if you haven't already\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import urllib\n",
    "\n",
    "# Create connection string for SQLAlchemy\n",
    "params = urllib.parse.quote_plus(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=localhost\\\\SQLEXPRESS;'\n",
    "    'Trusted_Connection=yes;'\n",
    "    'DATABASE=TestDB;'  # Specify database directly in connection\n",
    ")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={params}')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    customers = pd.read_sql_query(text('SELECT * FROM Customers'), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather video game data from unofficial overwatch API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "platform = \"pc\"\n",
    "user = \"wildwolf-199415\"\n",
    "\n",
    "url = f\"https://ow-api.com/v1/stats/{platform}/us/{user}/complete\"\n",
    "\n",
    "# Send the HTTP GET request to the API and parse the JSON response\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "save_folder = r\"C:\\Users\\Ryan\\Coding Projects\\SQL\"\n",
    "file_path = os.path.join(save_folder, 'overwatch_db.json')\n",
    "\n",
    "# Writing the json file \n",
    "with open(file_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "# Opening the json file\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Overwatch db from local json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "save_folder = r\"C:\\Users\\Ryan\\Coding Projects\\SQL\"\n",
    "file_path = os.path.join(save_folder, 'overwatch_db.json')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QP top heros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "temp_json = overwatch_db['quickPlayStats']['topHeroes']\n",
    "\n",
    "QP_Top_Heros = pd.DataFrame(temp_json)\n",
    "\n",
    "# Simple transpose to flip columns and rows\n",
    "QP_Top_Heros =  QP_Top_Heros.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QP career stats & postgres sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data from json\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "save_folder = r\"C:\\Users\\Ryan\\Coding Projects\\SQL\"\n",
    "file_path = os.path.join(save_folder, 'overwatch_db.json')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    overwatch_db = json.load(f)\n",
    "\n",
    "t_json = overwatch_db['quickPlayStats']['careerStats']\n",
    "\n",
    "# Changing dict/json to df and transposing it to flip columns and rows\n",
    "qp_career_stats = pd.DataFrame(t_json).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the dictionary from each column except for \"heroSpecific\" and make each dict pairing into their own column and row  \n",
    "\n",
    "def expand_dict(qp_career_stats):\n",
    "\n",
    "    # Put all column names from df into a list  \n",
    "    dict_columns = qp_career_stats.columns.to_list()\n",
    "\n",
    "    # Deleting heroSpecific from the column list (will be it's own db)\n",
    "    del dict_columns[4]\n",
    "\n",
    "    for col in dict_columns:\n",
    "        # Expand each dictionary column by using a pd.Series to create a new row for each dictionary it encounters with the dictionary keys as column names and values as the row data.pandas series\n",
    "        expanded_cols = qp_career_stats[col].apply(lambda x: pd.Series(x, dtype=\"object\"))\n",
    "        \n",
    "        # Rename new columns to include original column name as a prefix\n",
    "        expanded_cols = expanded_cols.add_prefix(f\"{col}_\")\n",
    "        \n",
    "        # Concatenate the expanded columns with the original DataFrame\n",
    "        qp_career_stats = pd.concat([qp_career_stats, expanded_cols], axis=1)\n",
    "\n",
    "        # Drop the original dictionary column\n",
    "        qp_career_stats = qp_career_stats.drop(columns=[col])\n",
    "    \n",
    "    # Putting the index as a regular column in DF \n",
    "    qp_career_stats = qp_career_stats.reset_index()\n",
    "    # Renaming that index column to hero\n",
    "    qp_career_stats.rename(columns={'index': 'Hero'}, inplace=True)\n",
    "    \n",
    "    return qp_career_stats\n",
    "\n",
    "qp_career_stats = expand_dict(qp_career_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql db cannot handle dictionaries in columns\n",
    "qp_career_stats = qp_career_stats.drop(columns=[\"heroSpecific\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into heroSpecific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Getting each hero and their specific stats into a df, getting rid of first observation because that's all heros\n",
    "hero_specific = qp_career_stats[['Hero', 'heroSpecific']].iloc[1:]\n",
    "\n",
    "# This loop puts each heros specific stats into a unique df named after the hero then saves it postgres\n",
    "for index, row in hero_specific.iterrows():\n",
    "    hero = row['Hero']\n",
    "    hero_dict = row['heroSpecific']\n",
    "\n",
    "    # Convert the dictionary into a new DataFrame\n",
    "    new_df = pd.DataFrame([hero_dict])\n",
    "\n",
    "    # Load variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create an SQLAlchemy engine\n",
    "    engine = create_engine(os.getenv(\"postgres_cs\"))\n",
    "\n",
    "    # name parameter sets the table name\n",
    "    new_df.to_sql(name = hero, con=engine, if_exists='replace', index=True)\n",
    "\n",
    "# End connection after loop instead of reactivating it over and over\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfering the pandas df to a postgres sql table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def df_to_postgresql(df, name):\n",
    "\n",
    "    # Load variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Create an SQLAlchemy engine\n",
    "    engine = create_engine(os.getenv(\"postgres_cs\"))\n",
    "\n",
    "    # name parameter sets the table name\n",
    "    df.to_sql(name = name, con=engine, if_exists='replace', index=True)\n",
    "\n",
    "    # After using the engine to interact with the database\n",
    "    engine.dispose()\n",
    "\n",
    "df_to_postgresql(qp_career_stats, \"qp_career_stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in postgres db to pandas db with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(os.getenv(\"postgres_cs\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df = pd.read_sql_query(\n",
    "        text(\"SELECT * FROM prints\"),\n",
    "        connection)\n",
    "\n",
    "# Using specific columns and conditions\n",
    "with engine.connect() as connection:\n",
    "    query = text(\"\"\"\n",
    "        SELECT card_number, name, color, print_completed\n",
    "        FROM prints\n",
    "        WHERE color = :color\"\"\")\n",
    "    \n",
    "    df_filtered = pd.read_sql_query(\n",
    "        query,\n",
    "        connection,\n",
    "        params={\"color\": \"Blue\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read, modify, then update SQL db\n",
    "\n",
    "Hypothetical: read in data from sql, then execute patron_contacting email script from 3d printing, results in new df called df_filtered, update existing sql db with df_filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from sql with specific paramters\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(os.getenv(\"postgres_cs\"))\n",
    "\n",
    "# read_sql_query simple \n",
    "with engine.connect() as connection:\n",
    "    df_filter = pd.read_sql_query(\n",
    "        text(\"\"\"SELECT * FROM prints\n",
    "                WHERE print_completed = 'X'\n",
    "                AND patron_contacted IS NULL\n",
    "                AND invalid_email IS NULL\n",
    "                AND picked_up IS NULL\"\"\"),\n",
    "        connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimicking sending out emails to patrons (this is the resulting df), results in df_filter\n",
    "\n",
    "import datetime as dt\n",
    "# Assuming `df_filter` is a pandas DataFrame\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"patron_contacted\"] = \"X\"\n",
    "\n",
    "today_str = dt.date.today().strftime(\"%m/%d/%y\") \n",
    "today_date = dt.datetime.strptime(today_str, \"%m/%d/%y\").date()\n",
    "df_filter.loc[df_filter[\"print_completed\"] == \"X\", \"contacted_date\"] = today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates sql db based on new inputs from df_filtered \n",
    "\n",
    "# Update query\n",
    "update_query = \"\"\"\n",
    "    UPDATE prints \n",
    "    SET patron_contacted = :patron_contacted,\n",
    "        contacted_date = :contacted_date,\n",
    "        invalid_email = :invalid_email\n",
    "    WHERE card_number = :card_number\n",
    "\"\"\"\n",
    "\n",
    "# Convert DataFrame rows to list of dictionaries for batch update (connection.execute used dictionaries as input for the parameters function)\n",
    "records_to_update = df_filter.to_dict('records')\n",
    "\n",
    "# Establish connection and execute updates\n",
    "with engine.begin() as connection:\n",
    "    try:\n",
    "        for record in records_to_update:\n",
    "            connection.execute(\n",
    "                text(update_query),\n",
    "                parameters=record)\n",
    "\n",
    "        print(f\"Successfully updated {len(records_to_update)} records in the database\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating database: {str(e)}\")\n",
    "        connection.rollback() # If an error occurs during the database update process, any partial or uncommitted changes made during the transaction are undone.\n",
    "        raise\n",
    "\n",
    "# Release and clean up all database connections managed by the SQLAlchemy engine \n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MYSQL db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_mysql_db(db): \n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # Define your credentials\n",
    "    username = 'root'\n",
    "    password = quote_plus(os.getenv(\"mysql_pass\"))  # Encodes the special characters\n",
    "    host = 'localhost' \n",
    "    port = 3306\n",
    "    database = db\n",
    "\n",
    "    # Create the connection string\n",
    "    connection_string = f\"mysql+pymysql://{username}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "\n",
    "    # Test the connection\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            # Use pd.read_sql_query to fetch data\n",
    "            df = pd.read_sql_query(text(\"SELECT * FROM city\"), connection)\n",
    "            print(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_mysql_db(\"sakila\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"uSGN6x6p@l@q\",\n",
    "    database=\"sakila\"\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM city\")\n",
    "results = cursor.fetchall()\n",
    "print(results)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Using pandas read_sql function is untested outside SQLAlchemy, so it'll throw a warning but still work\n",
    "# df = pd.read_sql(\"SELECT * FROM city\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random excel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prints = pd.read_excel(r\"C:\\Users\\Ryan\\Desktop\\EGR KDL Master 3D Printing List.xlsx\")\n",
    "\n",
    "df_to_postgresql(prints, \"prints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_save = (r\"C:\\Users\\Ryan\\Desktop\")\n",
    "file_name = f\"qp_career_stats.xlsx\" \n",
    "file_path = os.path.join(directory_save, file_name) \n",
    "\n",
    "qp_career_stats.to_excel(file_path , index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlalchemy + pandas is the way to go for interacting with any SQL db then modifying it with pandas    \n",
    "psycopg2=postgres  &  pymysql=mysql"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
